{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>audio/cat/胶州路口马路施工-53.wav</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audio/dog/胶州路口马路施工-53.wav</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>audio/pig/胶州路口马路施工-53.wav</td>\n",
       "      <td>pig</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        file class\n",
       "0  audio/cat/胶州路口马路施工-53.wav   cat\n",
       "1  audio/dog/胶州路口马路施工-53.wav   dog\n",
       "2  audio/pig/胶州路口马路施工-53.wav   pig"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import librosa\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1. 把音频读入并保存成数据集\n",
    "\n",
    "# 获取当前工作目录\n",
    "current_dir = os.getcwd()\n",
    "# 设置当前工作目录,现在可以使用相对路径来操作文件了\n",
    "os.chdir(current_dir)\n",
    "\n",
    "# 设置文件夹路径\n",
    "base_folder = \"audio\"\n",
    "\n",
    "# 获取所有子文件夹的名字\n",
    "subfolders = [f.name for f in os.scandir(base_folder) if f.is_dir()]\n",
    "\n",
    "# 初始化字典，用于存储每个文件夹中的音频文件\n",
    "data = defaultdict(list)\n",
    "\n",
    "# 遍历每个子文件夹，获取音频文件名并存储到字典中\n",
    "for subfolder in subfolders:\n",
    "    subfolder_path = os.path.join(base_folder, subfolder)\n",
    "    audio_files = [\n",
    "        os.path.join(subfolder_path, f.name)\n",
    "        for f in os.scandir(subfolder_path)\n",
    "        if f.is_file() and f.name.endswith(\".wav\")\n",
    "    ]\n",
    "\n",
    "    data[subfolder] = audio_files\n",
    "\n",
    "# 初始化一个空的列表，用于存储每个音频和其对应的类名\n",
    "rows = []\n",
    "\n",
    "# 遍历字典中的每个类别和对应的音频列表\n",
    "for class_name, audio_list in data.items():\n",
    "    for audio_name in audio_list:\n",
    "        rows.append({\"file\": audio_name, \"class\": class_name})\n",
    "\n",
    "# 将列表转换为DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# 输出DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'label'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "dataset_structure = {\"audio\": [], \"label\": []}\n",
    "dataset_origin = Dataset.from_dict(dataset_structure)\n",
    "\n",
    "\n",
    "for i in range(len(df)):\n",
    "    wav_path = df.loc[i, \"file\"]\n",
    "    data, sr = librosa.load(wav_path, sr=16000)\n",
    "    new_data = {\n",
    "        \"audio\": {\"array\": np.array(data), \"path\": wav_path, \"sampling_rate\": 16000},\n",
    "        \"label\": df.loc[i, \"class\"],\n",
    "    }\n",
    "\n",
    "    dataset_origin = dataset_origin.add_item(new_data)\n",
    "\n",
    "dataset_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b436cc4b6c0d40d2bd5aca227afc348a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import ClassLabel\n",
    "\n",
    "unique = df[\"class\"].unique()\n",
    "# 这个下面要用的！！！\n",
    "num_classes = len(unique)\n",
    "class_names = unique.tolist()\n",
    "\n",
    "target_transform = ClassLabel(names=class_names)\n",
    "\n",
    "# 使用dataset的.map方法来修改列的数据类型\n",
    "column_name = \"label\"\n",
    "dataset = dataset_origin.map(\n",
    "    lambda example: {column_name: target_transform.str2int(example[column_name])}\n",
    ")\n",
    "\n",
    "# 将数据集转换为DataFrame\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# # 将DataFrame保存为Parquet文件\n",
    "parquet_path = \"./dataset/new_dataset.parquet\"\n",
    "df.to_parquet(parquet_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, ASTModel\n",
    "import torch\n",
    "from datasets import Dataset, load_metric\n",
    "\n",
    "# 2. 开始训练，上面都是在处理数据集\n",
    "\n",
    "# model_checkpoint = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "model_checkpoint = r\"../checkpoint-95\"\n",
    "sampling_rate = 16000\n",
    "metric = load_metric(\"accuracy\")\n",
    "# metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_checkpoint)\n",
    "model = ASTModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9dc99221ee04f7b8b7bfff305e4ccae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3393ea78fea404d97d422f2e93a6f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ece0bee490c43c3aab339119e2ce2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, Features, Sequence, Value, ClassLabel\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "# 定义特征的数据类型\n",
    "features = Features(\n",
    "    {\n",
    "        \"audio\": {\n",
    "            \"array\": Sequence(feature=Value(dtype=\"float32\")),\n",
    "            \"path\": Value(dtype=\"string\"),\n",
    "            \"sampling_rate\": Value(dtype=\"int64\"),\n",
    "        },\n",
    "        \"label\": ClassLabel(num_classes=num_classes, names=class_names),\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"parquet\", data_files=r\"./dataset/new_dataset.parquet\", features=features\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=0.5 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m     split_dataset[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m split_dataset_test[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     10\u001b[0m     \u001b[39mreturn\u001b[39;00m split_dataset\n\u001b[0;32m---> 13\u001b[0m dataset \u001b[39m=\u001b[39m split_train_test_val(dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     14\u001b[0m dataset\n",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m, in \u001b[0;36msplit_train_test_val\u001b[0;34m(csv_dataset)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Rename the default \"test\" split to \"validation\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m split_dataset[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m split_dataset\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m split_dataset_test \u001b[39m=\u001b[39m split_dataset[\u001b[39m\"\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mtrain_test_split(\n\u001b[1;32m      6\u001b[0m     test_size\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m, seed\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m split_dataset[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m split_dataset_test[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m split_dataset[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m split_dataset_test[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/Audio-demo/lib/python3.8/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Audio-demo/lib/python3.8/site-packages/datasets/fingerprint.py:511\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    509\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m out \u001b[39m=\u001b[39m func(dataset, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    513\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Audio-demo/lib/python3.8/site-packages/datasets/arrow_dataset.py:4445\u001b[0m, in \u001b[0;36mDataset.train_test_split\u001b[0;34m(self, test_size, train_size, shuffle, stratify_by_column, seed, generator, keep_in_memory, load_from_cache_file, train_indices_cache_file_name, test_indices_cache_file_name, writer_batch_size, train_new_fingerprint, test_new_fingerprint)\u001b[0m\n\u001b[1;32m   4442\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[1;32m   4444\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 4445\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   4446\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{\u001b[39;00mn_samples\u001b[39m}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{\u001b[39;00mtest_size\u001b[39m}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{\u001b[39;00mtrain_size\u001b[39m}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4448\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4449\u001b[0m     )\n\u001b[1;32m   4451\u001b[0m load_from_cache_file \u001b[39m=\u001b[39m load_from_cache_file \u001b[39mif\u001b[39;00m load_from_cache_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_caching_enabled()\n\u001b[1;32m   4453\u001b[0m \u001b[39mif\u001b[39;00m generator \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=1, test_size=0.5 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "def split_train_test_val(csv_dataset):\n",
    "    split_dataset = csv_dataset.train_test_split(test_size=0.15, seed=42)\n",
    "    # Rename the default \"test\" split to \"validation\"\n",
    "    split_dataset[\"validation\"] = split_dataset.pop(\"test\")\n",
    "    split_dataset_test = split_dataset[\"validation\"].train_test_split(\n",
    "        test_size=0.5, seed=42\n",
    "    )\n",
    "    split_dataset[\"test\"] = split_dataset_test[\"test\"]\n",
    "    split_dataset[\"validation\"] = split_dataset_test[\"train\"]\n",
    "    return split_dataset\n",
    "\n",
    "\n",
    "dataset = split_train_test_val(dataset[\"train\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from IPython.display import Audio, display\n",
    "import numpy as np\n",
    "\n",
    "for _ in range(5):\n",
    "    rand_idx = random.randint(0, len(dataset[\"train\"]) - 1)\n",
    "    example = dataset[\"train\"][rand_idx]\n",
    "    audio = example[\"audio\"]\n",
    "\n",
    "    print(f'Label: {(example[\"label\"])}')\n",
    "    print(\n",
    "        f'Shape: {(np.array(audio[\"array\"])).shape}, sampling rate: {audio[\"sampling_rate\"]}'\n",
    "    )\n",
    "    display(Audio(audio[\"array\"], rate=audio[\"sampling_rate\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASTFeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"ASTFeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"max_length\": 1024,\n",
       "  \"mean\": -4.2677393,\n",
       "  \"num_mel_bins\": 128,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000,\n",
       "  \"std\": 4.5689974\n",
       "}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_duration = 5.0\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=int(feature_extractor.sampling_rate * max_duration),\n",
    "        truncation=True,\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "\n",
    "preprocess_function(dataset[\"train\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(\n",
    "    preprocess_function, remove_columns=[\"audio\"], batched=True\n",
    ")\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = len(label2id)\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=num_labels,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 4\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Audio-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
